<!DOCTYPE html>
<html lang="en">

<head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,opsz,wght@0,6..12,200..1000;1,6..12,200..1000&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&family=Source+Sans+3:ital,wght@0,200..900;1,200..900&family=Work+Sans:ital,wght@0,100..900;1,100..900&display=swap"
        rel="stylesheet">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>5 AI Safety books I recommend | Laura Hanu's Blog</title>
    <link rel="stylesheet" href="../style.css">

</head>

<body>
    <div class="container">
        <header>
            <a class="logo" href="../index.html">ðŸ‘¾</a>
            <nav>
                <a href="../index.html">about</a>
                <a href="../timeline.html" class="active">timeline</a>
                <a href="../blog.html" class="active">blog</a>
                <button id="darkModeToggle">ðŸŒš</button>
            </nav>
        </header>

        <main>
            <article class="blog-post">
                <h1 class="post-title">5 AI Safety books I recommend</h1>
                <p class="post-meta">August 19, 2019</p>

                <div class="post-content">
                    <p>The potential rise of superintelligent AI has gained a lot of traction in recent years and has
                        been deemed to be one of the most <a
                            href="https://intelligence.org/why-ai-safety/">important</a> discussions of our time.</p>

                    <blockquote>
                        <p>As a technologist, I see how AI and the fourth industrial revolution will impact every aspect
                            of people's lives.</p>
                        <footer>â€“ Fei-Fei Li</footer>
                    </blockquote>

                    <blockquote>
                        <p>The real risk with AI isn't malice but competence. A superintelligent AI will be extremely
                            good at accomplishing its goals, and if those goals aren't aligned with ours, we're in
                            trouble. You're probably not an evil ant-hater who steps on ants out of malice, but if
                            you're in charge of a hydroelectric green energy project and there's an anthill in the
                            region to be flooded, too bad for the ants. Let's not place humanity in the position of
                            those ants.</p>
                        <footer>â€“ Stephen Hawking</footer>
                    </blockquote>

                    <p>Here's a non-exhaustive list of non-fiction books I have found informative and relevant to the
                        field of AI Safety.</p>

                    <section class="book-recommendation">
                        <h2><a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies">1.
                                Superintelligence: Paths, Dangers, Strategies</a></h2>
                        <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1400884046l/20527133.jpg"
                            alt="Superintelligence book cover" width="200">
                        <h3>Why read it?</h3>
                        <p>This 2014 cult classic in the field of AI Safety has been one of the catalysts for the
                            popular concern around building superintelligent AI in the last few years.</p>
                        <p>Written in a time when the deep learning revolution had just started to take off, Nick
                            Bostrom's book focuses on the risks of developing machines that are 'vastly smarter' than
                            humans. Beginning with an exploration of 2 possible options: a 'slow takeoff' and a 'fast
                            takeoff', the book rigorously discusses and analyses the risks associated with building
                            superhuman AI.</p>
                        <h3>Why is it relevant to AI?</h3>
                        <p>This book gained quite a bit of media coverage due to the likes of <a
                                href="https://twitter.com/elonmusk/status/495759307346952192?lang=en">Elon Musk</a>, <a
                                href="https://qz.com/698334/bill-gates-says-these-are-the-two-books-we-should-all-read-to-understand-ai/">Bill
                                Gates</a>, and <a href="https://intelligence.org/2014/07/25/bostrom/">Stuart
                                Russell</a>, which then helped kickstart AI safety as a serious research field with
                            proper funding, such as Nick Bostrom's own institute at the University of Oxford, The Future
                            of Humanity Institute.</p>
                        <h3>Memorable quote</h3>
                        <blockquote>
                            <p>Let an ultraintelligent machine be defined as a machine that can far surpass all the
                                intellectual activities of any man however clever. Since the design of machines is one
                                of these intellectual activities, an ultraintelligent machine could design even better
                                machines; there would then unquestionably be an "intelligence explosion," and the
                                intelligence of man would be left far behind. Thus the first ultraintelligent machine is
                                the last invention that man need ever make, provided that the machine is docile enough
                                to tell us how to keep it under control.</p>
                        </blockquote>
                    </section>
                    <section class="book-recommendation">
                        <h2><a href="https://en.wikipedia.org/wiki/Life_3.0">2. Life 3.0</a></h2>
                        <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1522047003l/37857021._SY475_.jpg"
                            alt="Life 3.0 book cover" width="200">
                        <h3>Why read it?</h3>
                        <p>Although its topic is similar to Superintelligence, I found Max Tegmark's 2017 book to be a
                            more balanced view of the future of AI, albeit slightly on the optimistic side.</p>
                        <p>Despite the fact that both focus on medium to long term societal implications of
                            superintelligent AI, Tegmark tries to look at how we can enforce and maximise a positive
                            outcome, rather than focusing on the potential risks of AI, as described in Bostrom's
                            Superintelligence.</p>
                        <p>Life 3.0, in Tegmark's view, is a technological stage, where its lifeform designs both its
                            hardware and software (here software is the mechanism through which we process information
                            and make decisions and hardware is the body that stores the information and/or takes
                            action). Life 1.0 is the biological stage, where its hardware and software are entirely a
                            product of evolution, while Life 2.0 is the cultural stage with an evolved hardware and
                            partly designed software (e.g. humans can learn complex skills).</p>
                        <h3>Why is it relevant to AI?</h3>
                        <p>Apart from being a more readable, updated, introduction into the recent developments in AI
                            and AI Safety, Life 3.0 offers a range of scenarios and possibilities of how AI will impact
                            different aspects of humanity that go from the next few years to far into the distant
                            future, which is where his physics background comes most handy.</p>
                        <p>I also enjoyed his ventures into more metaphysical topics, such as consciousness or what
                            humanity's purpose might be in the presence of superintelligent AI. Ultimately, I think it
                            hits the right chord; it both terrifies and excites the reader in equal measures about the
                            impact this technology will have.</p>
                        <h3>Memorable quote</h3>
                        <blockquote>
                            <p>Without technology, our human extinction is imminent in the cosmic context of tens of
                                billions of years, rendering the entire drama of life in our Universe merely a brief and
                                transient flash of beauty, passion and meaning in a near eternity of meaninglessness
                                experienced by nobody.</p>
                        </blockquote>
                    </section>
                    <section class="book-recommendation">
                        <h2><a href="https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions">3. The
                                Structure of Scientific Revolutions</a></h2>
                        <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1396422530l/61539.jpg"
                            alt="The Structure of Scientific Revolutions book cover" width="200">
                        <h3>Why read it?</h3>
                        <p>Although this book was written in 1962, its take on the progress of science is still very
                            relevant today. Thomas Kuhn introduced the idea that science doesn't develop by accumulating
                            ideas and theories overtime, but rather by periods of slow development, interspersed with
                            periods of revolutionary ideas that partially or entirely replace the established paradigms,
                            a phenomenon which he called 'paradigm shifts'.</p>
                        <p>I found this a valuable read in order to better understand how scientific knowledge evolves
                            and how novel ideas initially get suppressed or ignored by the scientific community.</p>
                        <p>This particularly amusing <a
                                href="https://www.goodreads.com/review/show/516409229?book_show_action=true&from_review_page=1">review</a>
                            compares the way scientists hold onto traditional ideas to the way people behave in romantic
                            relationships.</p>
                        <h3>Why is it relevant to AI?</h3>
                        <p>Given that we are in the midst of an AI revolution at the moment, this book might offer
                            insight into how this revolution happened and how it will proceed. It might help to
                            understand why Deep learning has taken off and why it might take awhile until alternative,
                            more novel approaches replace it.</p>
                        <h3>Memorable quote</h3>
                        <blockquote>
                            <p>What man sees depends both upon what he looks at and also upon what his previous
                                visual-conception experience has taught him to see.</p>
                        </blockquote>
                    </section>

                    <section class="book-recommendation">
                        <h2><a href="https://en.wikipedia.org/wiki/Homo_Deus:_A_Brief_History_of_Tomorrow">4. Homo Deus:
                                A Brief History of Tomorrow</a></h2>
                        <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1468760805l/31138556._SY475_.jpg"
                            alt="Homo Deus book cover" width="200">
                        <h3>Why read it?</h3>
                        <p>Yuval Noah Harari, a history professor at the Hebrew University in Jerusalem, became famous
                            with his first novel, <a
                                href="https://en.wikipedia.org/wiki/Sapiens:_A_Brief_History_of_Humankind">Sapiens: A
                                Brief History of Humankind</a>. In his 2nd book, Harari focuses on the future of
                            humanity and our increasingly tight relationship with technology, which could result in our
                            evolution from homo sapiens to 'homo deus'. In his view, our quest to re-design our minds
                            and bodies to eventually defy death is comparable to a quest to achieve divinity.</p>
                        <p>I particularly liked its focus on biotechnology, which, from what I have seen, is rarely part
                            of the recent conversation around AI safety.</p>
                        <h3>Why is it relevant to AI?</h3>
                        <p>I have found Yuval Noah Harari's books to be quite polarising among AI practitioners. Coming
                            from a history/humanitarian background he does seem to hold quite strong speculative
                            opinions that are almost reductive at points, however, they are usually at least great
                            conversation starters.</p>
                        <p>Whether you agree with him or not, he does raise important concerns about our identity and
                            our sense of agency in an age of dataism, as highly intelligent AI could end up knowing us
                            better than we know ourselves.</p>
                        <p>Are we only going to lose more and more control over our own decisions, becoming merely data
                            points to be fed in ever increasingly intelligent algorithms? Are we making ourselves
                            obsolete in the process of building AGI?</p>
                        <h3>Memorable quote</h3>
                        <blockquote>
                            <p>In fact, as time goes by, it becomes easier and easier to replace humans with computer
                                algorithms, not merely because the algorithms are getting smarter, but also because
                                humans are professionalising. Ancient hunter-gatherers mastered a very wide variety of
                                skills in order to survive, which is why it would be immensely difficult to design a
                                robotic hunter-gatherer. Such a robot would have to know how to prepare spear points
                                from flint stones, how to find edible mushrooms in a forest, how to use medicinal herbs
                                to bandage a wound, how to track down a mammoth and how to coordinate a charge with a
                                dozen other hunters. However, over the last few thousand years we humans have been
                                specialising. A taxi driver or a cardiologist specialises in a much narrower niche than
                                a hunter-gatherer, which makes it easier to replace them with AI.</p>
                        </blockquote>
                    </section>

                    <section class="book-recommendation">
                        <h2><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">5. Thinking, Fast and
                                Slow</a></h2>
                        <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1317793965l/11468377.jpg"
                            alt="Thinking, Fast and Slow book cover" width="200">
                        <h3>Why read it?</h3>
                        <p>In this book, Nobel Prize laureate, Daniel Kahneman, explores the way we think and the
                            multiple kinds of unconscious biases we have when making decisions. His theories are a
                            product of decades of research into behavioural economics and give insight into when we
                            should and shouldn't trust our intuitions.</p>
                        <p>Kahneman introduces the "dual-process" model of the brain, namely 2 very different ways of
                            thinking, which he calls System 1 and System 2. System 1 is our fast way of thinking that
                            works by pattern recognition, optimised by millions of years of evolution. System 2, on the
                            other hand, is our slow, rational way of thinking.</p>
                        <p>This might be one of the most informative books I have read and it definitely changed the way
                            I look at my own decisions.</p>
                        <h3>Why is it relevant to AI?</h3>
                        <p>When building AI systems or even AGI, we are likely to incorporate our own biases into it.
                            The more we know about them and realise they are there, the more we can build a safe and
                            fair AI.</p>
                        <p>If the algorithm seems biased, the issue is likely to be upstream with the data the algorithm
                            was trained on or the model design specified by engineers or researchers building the AI
                            model.</p>
                        <h3>Memorable quote</h3>
                        <blockquote>
                            <p>A reliable way to make people believe in falsehoods is frequent repetition, because
                                familiarity is not easily distinguished from truth. Authoritarian institutions and
                                marketers have always known this fact.</p>
                        </blockquote>
                    </section>
                    <section class="additional-resources">
                        <h2>Other resources</h2>
                        <ul>
                            <li><a
                                    href="https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1">Deepmind
                                    on building safe AI: specification, robustness and assurance</a></li>
                            <li><a
                                    href="https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/">80000
                                    hours on positively shaping AI</a></li>
                            <li><a
                                    href="https://web.archive.org/web/20170311011724/https://futureoflife.org/ai-open-letter">Open
                                    letter on robust and beneficial AI</a></li>
                            <li><a href="https://intelligence.org/why-ai-safety/">MIRI on the need for AI Safety</a>
                            </li>
                            <li><a href="https://openai.com/blog/ai-safety-needs-social-scientists/">OpenAI on AI
                                    Safety</a></li>
                            <li><a href="https://nickbostrom.com/ethics/artificial-intelligence.pdf">Nick Bostrom on
                                    ethics of AI</a></li>
                            <li><a href="https://arxiv.org/pdf/1805.01109.pdf">Paper on AFI Safety literature review</a>
                            </li>
                            <li><a href="https://arxiv.org/pdf/1606.06565.pdf">Paper on Concrete Problems in AI
                                    Safety</a></li>
                            <li><a href="https://futureoflife.org/data/documents/research_survey.pdf">Future of life: A
                                    survey of research questions for robust and beneficial AI</a></li>
                        </ul>
                    </section>

                    <p>To end, thought I'd leave this quote here:</p>

                    <blockquote>
                        <p>The saddest aspect of life right now is that science gathers knowledge faster than society
                            gathers wisdom.</p>
                        <footer>â€“ Isaac Asimov</footer>
                    </blockquote>
                </div>
            </article>

        </main>
        <footer class="copyright-footer">
            <div class="social-links">
                <div class="socials">
                <a href="https://github.com/laurahanu" target="_blank" rel="noopener noreferrer">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path
                            d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" />
                    </svg>
                </a>
                </div>
                <a href="https://www.linkedin.com/in/laura-hanu-a0941691/" target="_blank" rel="noopener noreferrer">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path
                            d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z" />
                    </svg>
                </a>
                <a href="https://x.com/HanuLaura" target="_blank" rel="noopener noreferrer">
                    <svg xmlns="http://www.w3.org/2000/svg" width="1200" height="1227" viewBox="0 0 1200 1227" fill="none">
                        <path d="M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z"/>
                        </svg>
                </a>
                <a href="https://scholar.google.com/citations?user=07hm3DYAAAAJ&hl=en" target="_blank"
                    rel="noopener noreferrer">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path
                            d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z" />
                    </svg>
                </a>
            </div>
            <p>&copy; 2024 Laura Hanu</p>
        </footer> 
    </div>
    <script src="../script.js"></script>
</body>
</html>